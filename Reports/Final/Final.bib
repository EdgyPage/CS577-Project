
@misc{stanczak_survey_2021,
	title = {A Survey on Gender Bias in Natural Language Processing},
	url = {http://arxiv.org/abs/2112.14168},
	abstract = {Language can be used as a means of reproducing and enforcing harmful stereotypes and biases and has been analysed as such in numerous research. In this paper, we present a survey of 304 papers on gender bias in natural language processing. We analyse definitions of gender and its categories within social sciences and connect them to formal definitions of gender bias in {NLP} research. We survey lexica and datasets applied in research on gender bias and then compare and contrast approaches to detecting and mitigating gender bias. We find that research on gender bias suffers from four core limitations. 1) Most research treats gender as a binary variable neglecting its fluidity and continuity. 2) Most of the work has been conducted in monolingual setups for English or other high-resource languages. 3) Despite a myriad of papers on gender bias in {NLP} methods, we find that most of the newly developed algorithms do not test their models for bias and disregard possible ethical considerations of their work. 4) Finally, methodologies developed in this line of research are fundamentally flawed covering very limited definitions of gender bias and lacking evaluation baselines and pipelines. We suggest recommendations towards overcoming these limitations as a guide for future research.},
	number = {{arXiv}:2112.14168},
	publisher = {{arXiv}},
	author = {Stanczak, Karolina and Augenstein, Isabelle},
	urldate = {2023-10-01},
	date = {2021-12-28},
	eprinttype = {arxiv},
	eprint = {2112.14168 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
	file = {arXiv.org Snapshot:C\:\\Users\\myfir\\Zotero\\storage\\NHK7BI8T\\2112.html:text/html;Full Text PDF:C\:\\Users\\myfir\\Zotero\\storage\\C4Y5BJ5F\\Stanczak and Augenstein - 2021 - A Survey on Gender Bias in Natural Language Proces.pdf:application/pdf},
}

@inproceedings{sun_mitigating_2019,
	location = {Florence, Italy},
	title = {Mitigating Gender Bias in Natural Language Processing: Literature Review},
	url = {https://aclanthology.org/P19-1159},
	doi = {10.18653/v1/P19-1159},
	shorttitle = {Mitigating Gender Bias in Natural Language Processing},
	abstract = {As Natural Language Processing ({NLP}) and Machine Learning ({ML}) tools rise in popularity, it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes. Although {NLP} models have shown success in modeling various applications, they propagate and may even amplify gender bias found in text corpora. While the study of bias in artificial intelligence is not new, methods to mitigate gender bias in {NLP} are relatively nascent. In this paper, we review contemporary studies on recognizing and mitigating gender bias in {NLP}. We discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias. Furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods. Finally, we discuss future studies for recognizing and mitigating gender bias in {NLP}.},
	eventtitle = {{ACL} 2019},
	pages = {1630--1640},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Sun, Tony and Gaut, Andrew and Tang, Shirlyn and Huang, Yuxin and {ElSherief}, Mai and Zhao, Jieyu and Mirza, Diba and Belding, Elizabeth and Chang, Kai-Wei and Wang, William Yang},
	urldate = {2023-10-02},
	date = {2019-07},
	file = {Full Text PDF:C\:\\Users\\myfir\\Zotero\\storage\\CJSJAZWB\\Sun et al. - 2019 - Mitigating Gender Bias in Natural Language Process.pdf:application/pdf},
}

@inproceedings{madaan_analyze_2018,
	title = {Analyze, Detect and Remove Gender Stereotyping from Bollywood Movies},
	url = {https://proceedings.mlr.press/v81/madaan18a.html},
	abstract = {The presence of gender stereotypes in many aspects of society is a well-known phenomenon. In this paper, we focus on studying such stereotypes and bias in Hindi movie industry ({\textbackslash}it Bollywood) and propose an algorithm to remove these stereotypes from text. We analyze movie plots and posters for all movies released since 1970. The gender bias is detected by semantic modeling of plots at sentence and intra-sentence level. Different features like occupation, introductions, associated actions and descriptions are captured to show the pervasiveness of gender bias and stereotype in movies. Using the derived semantic graph, we compute centrality of each character and observe similar bias there. We also show that such bias is not applicable for movie posters where females get equal importance even though their character has little or no impact on the movie plot. The silver lining is that our system was able to identify 30 movies over last 3 years where such stereotypes were broken. The next step, is to generate debiased stories. The proposed debiasing algorithm extracts gender biased graphs from unstructured piece of text in stories from movies and de-bias these graphs to generate plausible unbiased stories.},
	eventtitle = {Conference on Fairness, Accountability and Transparency},
	pages = {92--105},
	booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
	publisher = {{PMLR}},
	author = {Madaan, Nishtha and Mehta, Sameep and Agrawaal, Taneea and Malhotra, Vrinda and Aggarwal, Aditi and Gupta, Yatin and Saxena, Mayank},
	urldate = {2023-10-03},
	date = {2018-01-21},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Full Text PDF:C\:\\Users\\myfir\\Zotero\\storage\\6J2RX5ZZ\\Madaan et al. - 2018 - Analyze, Detect and Remove Gender Stereotyping fro.pdf:application/pdf},
}

@misc{fast_shirtless_2016,
	title = {Shirtless and Dangerous: Quantifying Linguistic Signals of Gender Bias in an Online Fiction Writing Community},
	url = {http://arxiv.org/abs/1603.08832},
	doi = {10.48550/arXiv.1603.08832},
	shorttitle = {Shirtless and Dangerous},
	abstract = {Imagine a princess asleep in a castle, waiting for her prince to slay the dragon and rescue her. Tales like the famous Sleeping Beauty clearly divide up gender roles. But what about more modern stories, borne of a generation increasingly aware of social constructs like sexism and racism? Do these stories tend to reinforce gender stereotypes, or counter them? In this paper, we present a technique that combines natural language processing with a crowdsourced lexicon of stereotypes to capture gender biases in fiction. We apply this technique across 1.8 billion words of fiction from the Wattpad online writing community, investigating gender representation in stories, how male and female characters behave and are described, and how authors' use of gender stereotypes is associated with the community's ratings. We find that male over-representation and traditional gender stereotypes (e.g., dominant men and submissive women) are common throughout nearly every genre in our corpus. However, only some of these stereotypes, like sexual or violent men, are associated with highly rated stories. Finally, despite women often being the target of negative stereotypes, female authors are equally likely to write such stereotypes as men.},
	number = {{arXiv}:1603.08832},
	publisher = {{arXiv}},
	author = {Fast, Ethan and Vachovsky, Tina and Bernstein, Michael S.},
	urldate = {2023-10-04},
	date = {2016-03-29},
	eprinttype = {arxiv},
	eprint = {1603.08832 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Social and Information Networks},
	file = {arXiv Fulltext PDF:C\:\\Users\\myfir\\Zotero\\storage\\NJZVYIZZ\\Fast et al. - 2016 - Shirtless and Dangerous Quantifying Linguistic Si.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\myfir\\Zotero\\storage\\GY7LTQ8Y\\1603.html:text/html},
}

@misc{zhao_learning_2018,
	title = {Learning Gender-Neutral Word Embeddings},
	url = {http://arxiv.org/abs/1809.01496},
	doi = {10.48550/arXiv.1809.01496},
	abstract = {Word embedding models have become a fundamental component in a wide range of Natural Language Processing ({NLP}) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of {GloVe} ({GN}-{GloVe}). Quantitative and qualitative experiments demonstrate that {GN}-{GloVe} successfully isolates gender information without sacrificing the functionality of the embedding model.},
	number = {{arXiv}:1809.01496},
	publisher = {{arXiv}},
	author = {Zhao, Jieyu and Zhou, Yichao and Li, Zeyu and Wang, Wei and Chang, Kai-Wei},
	urldate = {2023-10-18},
	date = {2018-08-29},
	eprinttype = {arxiv},
	eprint = {1809.01496 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\myfir\\Zotero\\storage\\WXTRWAC8\\Zhao et al. - 2018 - Learning Gender-Neutral Word Embeddings.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\myfir\\Zotero\\storage\\KJU4IUMI\\1809.html:text/html},
}

@misc{levy_collecting_2021,
	title = {Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation},
	url = {http://arxiv.org/abs/2109.03858},
	abstract = {Recent works have found evidence of gender bias in models of machine translation and coreference resolution using mostly synthetic diagnostic datasets. While these quantify bias in a controlled experiment, they often do so on a small scale and consist mostly of artificial, out-of-distribution sentences. In this work, we find grammatical patterns indicating stereotypical and non-stereotypical gender-role assignments (e.g., female nurses versus male dancers) in corpora from three domains, resulting in a first large-scale gender bias dataset of 108K diverse real-world English sentences. We manually verify the quality of our corpus and use it to evaluate gender bias in various coreference resolution and machine translation models. We find that all tested models tend to over-rely on gender stereotypes when presented with natural inputs, which may be especially harmful when deployed in commercial systems. Finally, we show that our dataset lends itself to finetuning a coreference resolution model, finding it mitigates bias on a held out set. Our dataset and models are publicly available at www.github.com/{SLAB}-{NLP}/{BUG}. We hope they will spur future research into gender bias evaluation mitigation techniques in realistic settings.},
	number = {{arXiv}:2109.03858},
	publisher = {{arXiv}},
	author = {Levy, Shahar and Lazar, Koren and Stanovsky, Gabriel},
	urldate = {2023-11-02},
	date = {2021-09-10},
	eprinttype = {arxiv},
	eprint = {2109.03858 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\myfir\\Zotero\\storage\\4GQSGKHS\\2109.html:text/html;Full Text PDF:C\:\\Users\\myfir\\Zotero\\storage\\ESV5C3DT\\Levy et al. - 2021 - Collecting a Large-Scale Gender Bias Dataset for C.pdf:application/pdf},
}

@software{noauthor_bug_2023,
	title = {{BUG} Dataset},
	rights = {{MIT}},
	url = {https://github.com/SLAB-NLP/BUG},
	abstract = {A Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation, Levy et al., Findings of {EMNLP} 2021},
	publisher = {{SLAB}-{NLP}},
	urldate = {2023-11-02},
	date = {2023-06-14},
	note = {original-date: 2021-08-28T09:53:48Z},
}

@software{noauthor_learning_2023,
	title = {Learning Gender-Neutral Word Embeddings ({EMNLP} 2018)},
	rights = {Apache-2.0},
	url = {https://github.com/uclanlp/gn_glove},
	abstract = {Learning Gender-Neutral Word Embeddings},
	publisher = {Natural Language Processing @{UCLA}},
	urldate = {2023-12-02},
	date = {2023-11-13},
	note = {original-date: 2018-08-22T21:57:55Z},
}

@software{noauthor_stanfordnlpglove_2023,
	title = {stanfordnlp/{GloVe}},
	rights = {Apache-2.0},
	url = {https://github.com/stanfordnlp/GloVe},
	abstract = {Software in C and data files for the popular {GloVe} model for distributed word representations, a.k.a. word vectors or embeddings},
	publisher = {Stanford {NLP}},
	urldate = {2023-12-02},
	date = {2023-11-30},
	note = {original-date: 2015-09-01T17:21:18Z},
}

@software{noauthor_gap_2023,
	title = {{GAP} Coreference Dataset},
	rights = {Apache-2.0},
	url = {https://github.com/google-research-datasets/gap-coreference},
	abstract = {{GAP} is a gender-balanced dataset containing 8,908 coreference-labeled pairs of (ambiguous pronoun, antecedent name), sampled from Wikipedia for the evaluation of coreference resolution in practical applications.},
	publisher = {Google Research Datasets},
	urldate = {2023-12-02},
	date = {2023-11-24},
	note = {original-date: 2018-10-18T17:48:21Z},
}

@online{noauthor_stanford_nodate,
	title = {The Stanford Natural Language Processing Group},
	url = {https://nlp.stanford.edu/projects/coref.shtml},
	urldate = {2023-12-02},
	file = {The Stanford Natural Language Processing Group:C\:\\Users\\myfir\\Zotero\\storage\\IU8EWZNJ\\coref.html:text/html},
}

@book{noauthor_king_1989,
	title = {The King James Version of the Bible},
	rights = {Public domain in the {USA}.},
	url = {https://www.gutenberg.org/ebooks/10},
	urldate = {2023-12-02},
	date = {1989-08-01},
	keywords = {Bible},
}

@book{melville_moby_2001,
	title = {Moby Dick; Or, The Whale},
	rights = {Public domain in the {USA}.},
	url = {https://www.gutenberg.org/ebooks/2701},
	author = {Melville, Herman},
	urldate = {2023-12-02},
	date = {2001-07-01},
	keywords = {Adventure stories, Ahab, Captain (Fictitious character) -- Fiction, Mentally ill -- Fiction, Psychological fiction, Sea stories, Ship captains -- Fiction, Whales -- Fiction, Whaling -- Fiction, Whaling ships -- Fiction},
}
